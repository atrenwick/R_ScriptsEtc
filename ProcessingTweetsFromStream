# import libraries
library(tidyr) ; library(lubridate) ; library(rjson) ; library(jsonlite) ; library(tibble) ; library(beepr); library(tm) ; library(textclean) ; library(dplyr) ; library(stringr)

theCount <- 0 # set count of tweets to 0
FileNames <- "" # wipe list of filenames, if any
TweetCount <- 0 # set sum of counts to 0
Step5 <- "" # set this variable to blank
ImportDay <- as.integer(format(Sys.Date(), "%d")) -1 ### set as yesterday, as have 24h of tweets from yesterday
if ((ImportDay <10)){
  ImportDay <- paste(0,ImportDay, sep = "")} # if day is less than 10, add leading 0
RealMonth <- as.integer(format(Sys.Date(), "%m"))
if ((RealMonth <10)){
  RealMonth <- paste(0,RealMonth, sep = "")} # if month is less than 10, add leading 0

thisPath <- paste("/Users/me/Downloads/2021-", RealMonth, "-", ImportDay, "-FR-json/", sep="") # specify path where .json files are 
files <- list.files(path=thisPath, pattern="*.json", full.names=TRUE, recursive=FALSE) # list files in specified folder
{
  for (i in 1:length(files)){ 
    Step1 <- as_tibble(stream_in(file(files[i]))) ##Stream in
    col_order <- c("created_at", "id", "text", "lang") # set column order; non-specified columns will be deleted
    Step2 <- Step1[, col_order] # put columns in specified order
    Step2 <-Step2[Step2$lang == "fr", ] ##Filter to leave only FR tweets
    try((Step2$new <- parse_date_time(Step2$created_at, c("a b d  HMS z Y"))), outFile=Step2$new) # try parsing time as YYYYMMDD hhmmss timezone code timezone offset
    ## divide up according to results of this
    Step3A <- Step2[which(!is.na(Step2$new) ),] # if field 'new' is not empty, put this row in Step3A
    Step3B <- Step2[which(is.na(Step2$new) ),] # if field 'new' is empty, put this row in Step3B
    if (dim(Step2)[1] != dim(Step3A)[1] + dim(Step3B)[1]) {beep(2) ; break} # if sum of rows in ≠ sum of rows out, then break, as error needs to be fixed.
    try((Step3B$new <- ymd_hms(Step3B$created_at)), outFile=Step3B$new) # try converting to YMD-HMS format
    zzParsed <- rbind(Step3A, Step3B) # combine processed dataframes
    Step4 <- separate(zzParsed, new, c("Date", "Time"), sep=" ") # separate timestamp into date and time columns
    Step4 <- separate(Step4, Date, c("YYYY", "MM", "DD"), sep="-") # separate date into YYYY, MM, DD columns
    Step4 <- separate(Step4, Time, c("hh", "mm", "ss"), sep=":") # separate time column into HH, mm, SS columns
    Step4 <- Step4[-c(1)] # remove column 1
    Step5 <- rbind(Step5, Step4) # bind processed results to previously processed results (if any
    cat("Completed", i, "of", length(files), "Name", files[i], "\n") # print number of file processed, name
    thisCount <- dim(Step1)[1] # get number of tweets added
    TweetCount <- rbind(thisCount, TweetCount) # add number of tweets to current sum of tweets added
    theCount <- thisCount + theCount # calculate new sum of tweets
    cat("+", thisCount, "->", theCount, "\n") ## print total number of tweets 
  }
  Step5 <- unique.data.frame(Step5) # remove any non-unique tweets
  Step6 <- Step5[!duplicated(Step5$id),] # remove any tweets with duplicate IDs
   
Step6$text <- gsub("http.+", "", Step6$text) # remove URLs
Step6$text <- gsub("#.+? ", "", Step6$text) # remove hashtags
Step6$text <- gsub("@.+? ", "", Step6$text) # remove twitter handles
Step6$text <- gsub("\'|’", "\' ", Step6$text) ## space elisions #
Step6$text <- gsub("_", "", Step6$text) ## remove double spaces
Step6$text <- gsub("-|–", "-", Step6$text) ## remove double spaces
Step6$text <- gsub("[\"“‘«]", "\"", Step6$text) # reokace opening quote variations with std double "
Step6$text <- gsub("[”’»]", "\"", Step6$text) # idem for closing
Step6$text <- gsub("[0-9]+", "", Step6$text) # remove numbers
Step6$text <- gsub(",", "", Step6$text) # remove commas 
Step6$text <- gsub("\\.", "", Step6$text) # remove periods/full stops
Step6$text <- gsub("tco.*ing.", "", Step6$text) # remove twitter shortUrls
Step6$text <- gsub("(f|ht)t(ps|p):.*(\\s|$)", "", Step6$text, ignore.case = TRUE) # remove further URL/FTP address variants
Step6$text <- gsub("@[a-z,0-9,:punct:,_]*?\\s", "", Step6$text, ignore.case = TRUE) # remove additional twitter handles
Step6$text <- add_comma_space(Step6$text) # add spaces after any remaining commas
Step6$text <- replace_non_ascii(Step6$text) # remove non-ascii text
Step6$text <- replace_hash(Step6$text) # remove hash codes
Step6$text <- replace_html(Step6$text) # remove html codes
Step6$text <- replace_emoji(Step6$text) # remove emoji
Step6$text <- replace_white(Step6$text) # remove redundant whitespace

  beep(1)} # make an audible beep when done
TweetStreamName <- paste("/Users/me/Downloads/2021-", RealMonth, "-", ImportDay, "-FR-LiveTweets.csv", sep = "") # set name of output file
KeyListName <- paste("/Users/me/Downloads/2021-", RealMonth, "-", ImportDay, "-Keylist.csv", sep = "") # set name of keylist file
write.csv((Step6[c(1,4,5,6,7,8,9)]), KeyListName) # print list of tweet IDs, to serve as backup and allow easy buildiing/rebuilding of dataset
write.csv((Step6[c(1,2)]), TweetStreamName) # print processed tweets + IDs to csv file


